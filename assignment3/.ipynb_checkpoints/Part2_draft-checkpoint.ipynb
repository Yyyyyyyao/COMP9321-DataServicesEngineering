{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wound-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import ensemble\n",
    "import math\n",
    "import datetime\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-miracle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "brutal-angle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_names(x):\n",
    "    json_data = json.loads(x)\n",
    "    list_genres = []\n",
    "    for token in json_data:\n",
    "        list_genres.append(token['name'])\n",
    "    return list_genres\n",
    "\n",
    "def format_training_data(df_training, df_validation):\n",
    "    training_dataset = pd.DataFrame(columns=['movie_id'])\n",
    "    validation_dataset = pd.DataFrame(columns=['movie_id'])\n",
    "    # movie_id:\n",
    "    training_dataset['movie_id'] = df_training['movie_id']\n",
    "    validation_dataset['movie_id'] = df_validation['movie_id']\n",
    "    \n",
    "    # revenue:\n",
    "    training_dataset['revenue'] = df_training['revenue']\n",
    "    validation_dataset['revenue'] = df_validation['revenue']\n",
    "    \n",
    "\n",
    "    # budget\n",
    "    training_dataset['budget'] = df_training['budget']\n",
    "    validation_dataset['budget'] = df_validation['budget']\n",
    "#     training_dataset['budget_cat_min_30000000'] = df_training['budget'].apply(lambda x: 1 if (x <=30000000) else 0)\n",
    "#     training_dataset['budget_cat_30000000_40000000'] = df_training['budget'].apply(lambda x: 1 if (x >30000000)&(x<=40000000) else 0)\n",
    "#     training_dataset['budget_cat_40000000_50000000'] = df_training['budget'].apply(lambda x: 1 if (x >40000000)&(x<=50000000) else 0)\n",
    "#     training_dataset['budget_cat_50000000_60000000'] = df_training['budget'].apply(lambda x: 1 if (x >50000000)&(x<=60000000) else 0)\n",
    "#     training_dataset['budget_cat_60000000_70000000'] = df_training['budget'].apply(lambda x: 1 if (x >60000000)&(x<=70000000) else 0)\n",
    "#     training_dataset['budget_cat_70000000_80000000'] = df_training['budget'].apply(lambda x: 1 if (x >70000000)&(x<=80000000) else 0)\n",
    "#     training_dataset['budget_cat_80000000_90000000'] = df_training['budget'].apply(lambda x: 1 if (x >80000000)&(x<=90000000) else 0)\n",
    "#     training_dataset['budget_cat_90000000_100000000'] = df_training['budget'].apply(lambda x: 1 if (x >90000000)&(x<=100000000) else 0)\n",
    "#     training_dataset['budget_cat_100000000_130000000'] = df_training['budget'].apply(lambda x: 1 if (x >100000000)&(x<=130000000) else 0)\n",
    "#     training_dataset['budget_cat_130000000_160000000'] = df_training['budget'].apply(lambda x: 1 if (x >130000000)&(x<=16000000) else 0)\n",
    "#     training_dataset['budget_cat_160000000_190000000'] = df_training['budget'].apply(lambda x: 1 if (x >160000000)&(x<=19000000) else 0)\n",
    "#     training_dataset['budget_cat_190000000_max'] = df_training['budget'].apply(lambda x: 1 if (x >=19000000) else 0)\n",
    "    \n",
    "#     validation_dataset['budget_cat_min_30000000'] = df_validation['budget'].apply(lambda x: 1 if (x <=30000000) else 0)\n",
    "#     validation_dataset['budget_cat_30000000_40000000'] = df_validation['budget'].apply(lambda x: 1 if (x >30000000)&(x<=40000000) else 0)\n",
    "#     validation_dataset['budget_cat_40000000_50000000'] = df_validation['budget'].apply(lambda x: 1 if (x >40000000)&(x<=50000000) else 0)\n",
    "#     validation_dataset['budget_cat_50000000_60000000'] = df_validation['budget'].apply(lambda x: 1 if (x >50000000)&(x<=60000000) else 0)\n",
    "#     validation_dataset['budget_cat_60000000_70000000'] = df_validation['budget'].apply(lambda x: 1 if (x >60000000)&(x<=70000000) else 0)\n",
    "#     validation_dataset['budget_cat_70000000_80000000'] = df_validation['budget'].apply(lambda x: 1 if (x >70000000)&(x<=80000000) else 0)\n",
    "#     validation_dataset['budget_cat_80000000_90000000'] = df_validation['budget'].apply(lambda x: 1 if (x >80000000)&(x<=90000000) else 0)\n",
    "#     validation_dataset['budget_cat_90000000_100000000'] = df_validation['budget'].apply(lambda x: 1 if (x >90000000)&(x<=100000000) else 0)\n",
    "#     validation_dataset['budget_cat_100000000_130000000'] = df_validation['budget'].apply(lambda x: 1 if (x >100000000)&(x<=130000000) else 0)\n",
    "#     validation_dataset['budget_cat_130000000_160000000'] = df_validation['budget'].apply(lambda x: 1 if (x >130000000)&(x<=16000000) else 0)\n",
    "#     validation_dataset['budget_cat_160000000_190000000'] = df_validation['budget'].apply(lambda x: 1 if (x >160000000)&(x<=19000000) else 0)\n",
    "#     validation_dataset['budget_cat_190000000_max'] = df_validation['budget'].apply(lambda x: 1 if (x >=19000000) else 0)\n",
    "    \n",
    "    # rating\n",
    "    training_dataset['rating'] = df_training['rating']\n",
    "    validation_dataset['rating'] = df_validation['rating']\n",
    "    \n",
    "    # homepage:\n",
    "    training_dataset['has_homepage'] = 0\n",
    "    training_dataset.loc[df_training['homepage'].isnull() == False, 'has_homepage'] = 1 #1 here means it has home page\n",
    "    \n",
    "    validation_dataset['has_homepage'] = 0\n",
    "    validation_dataset.loc[df_validation['homepage'].isnull() == False, 'has_homepage'] = 1 #1 here means it has home page\n",
    "    \n",
    "    # original_language:\n",
    "    unique_languages = list(df_training[\"original_language\"].apply(pd.Series).stack().unique())\n",
    "    for language in unique_languages :\n",
    "        training_dataset[language] = df_training['original_language'].apply(lambda x: 1 if x == language else 0)\n",
    "    for language in unique_languages :\n",
    "        validation_dataset[language] = df_validation['original_language'].apply(lambda x: 1 if x == language else 0)\n",
    "    \n",
    "    # Genres: # may improve by looking at how correlated to revenue\n",
    "#     unique_genres = list(training_dataset[\"genres\"].apply(pd.Series).stack().unique())\n",
    "\n",
    "    # get top 10 mean-revenue genres\n",
    "#     temp = df_training[['movie_id','revenue','genres']].copy()\n",
    "#     temp['genres'] = input_training['genres'].apply(lambda x: get_json_names(x))\n",
    "#     unique_genres = list(temp[\"genres\"].apply(pd.Series).stack().unique())\n",
    "\n",
    "#     dic={}\n",
    "#     for a in unique_genres:\n",
    "#         mask = temp['genres'].apply(lambda x: a in x)\n",
    "#         dic[a] = temp[mask]['revenue'].mean()\n",
    "\n",
    "#     t = pd.DataFrame.from_dict(dic, orient='index', columns=['mean_revenue']).reset_index().rename(columns={'index':'genre'})\n",
    "#     genres_list = list(t.sort_values(by=[\"mean_revenue\"],ascending=False)[\"genre\"])[:10]\n",
    "    \n",
    "    \n",
    "    # get  10 most appearing\n",
    "    temp = input_training[['movie_id','revenue','genres']].copy()\n",
    "    temp['genres'] = temp['genres'].apply(lambda x: get_json_names(x))\n",
    "    genres_list = list(temp['genres'].apply(pd.Series).stack().value_counts().index)[:10]\n",
    "    \n",
    "    \n",
    "#     genres_list=['Drama','Action','Comedy','Thriller','Adventure','Crime','Romance','Science Fiction','Family','Fantasy']\n",
    "    training_dataset['genres'] = df_training['genres'].apply(lambda x: get_json_names(x))\n",
    "    for genres in genres_list :\n",
    "        training_dataset['genre_'+genres]=training_dataset['genres'].apply(lambda x: 1 if genres in x else 0)\n",
    "    training_dataset = training_dataset.drop(['genres'], axis=1)\n",
    "\n",
    "    validation_dataset['genres'] = df_validation['genres'].apply(lambda x: get_json_names(x))\n",
    "    for genres in genres_list :\n",
    "        validation_dataset['genre_'+genres]=validation_dataset['genres'].apply(lambda x: 1 if genres in x else 0)\n",
    "    validation_dataset = validation_dataset.drop(['genres'], axis=1)\n",
    "\n",
    "    # Production company\n",
    "#     production_company_list = ['Warner Bros.', 'Universal Pictures', 'Paramount Pictures', 'Twentieth Century Fox Film Corporation', 'Columbia Pictures']\n",
    "    # top 10 companies who produce most movies\n",
    "    temp = input_training[['movie_id','revenue','production_companies']].copy()\n",
    "    temp['production_companies'] = input_training['production_companies'].apply(lambda x: get_json_names(x))\n",
    "    production_company_list = list(temp['production_companies'].apply(pd.Series).stack().value_counts().index)[:10]\n",
    "    \n",
    "    training_dataset['production_companies'] = df_training['production_companies'].apply(lambda x: get_json_names(x))\n",
    "    for company in production_company_list :\n",
    "        training_dataset['company_'+company]=training_dataset['production_companies'].apply(lambda x: 1 if company in x else 0)\n",
    "    training_dataset = training_dataset.drop(['production_companies'], axis=1)\n",
    "    \n",
    "    validation_dataset['production_companies'] = df_validation['production_companies'].apply(lambda x: get_json_names(x))\n",
    "    for company in production_company_list :\n",
    "        validation_dataset['company_'+company]=validation_dataset['production_companies'].apply(lambda x: 1 if company in x else 0)\n",
    "    validation_dataset = validation_dataset.drop(['production_companies'], axis=1)\n",
    "    \n",
    "    \n",
    "    # crew \n",
    "    temp = df_training[['movie_id','revenue','crew']].copy()\n",
    "    temp['crew'] = temp['crew'].apply(lambda x: get_json_names(x))\n",
    "    crew_list = list(temp['crew'].apply(pd.Series).stack().value_counts().index)[:10]\n",
    "    \n",
    "    training_dataset['crew'] = df_training['crew'].apply(lambda x: get_json_names(x))\n",
    "    for crew in crew_list :\n",
    "        training_dataset['crew_'+crew]=training_dataset['crew'].apply(lambda x: 1 if crew in x else 0)\n",
    "    training_dataset = training_dataset.drop(['crew'], axis=1)\n",
    "    \n",
    "    validation_dataset['crew'] = df_validation['crew'].apply(lambda x: get_json_names(x))\n",
    "    for crew in crew_list :\n",
    "        validation_dataset['crew_'+crew]=validation_dataset['crew'].apply(lambda x: 1 if crew in x else 0)\n",
    "    validation_dataset = validation_dataset.drop(['crew'], axis=1)\n",
    "    \n",
    "    # runtime\n",
    "#     training_dataset['runtime'] = df_training['runtime']\n",
    "#     validation_dataset['runtime'] = df_validation['runtime']\n",
    "    training_dataset['runtime_cat_min_60'] = df_training['runtime'].apply(lambda x: 1 if (x <=60) else 0)\n",
    "    training_dataset['runtime_cat_61_80'] = df_training['runtime'].apply(lambda x: 1 if (x >60)&(x<=80) else 0)\n",
    "    training_dataset['runtime_cat_81_100'] = df_training['runtime'].apply(lambda x: 1 if (x >80)&(x<=100) else 0)\n",
    "    training_dataset['runtime_cat_101_120'] = df_training['runtime'].apply(lambda x: 1 if (x >100)&(x<=120) else 0)\n",
    "    training_dataset['runtime_cat_121_140'] = df_training['runtime'].apply(lambda x: 1 if (x >120)&(x<=140) else 0)\n",
    "    training_dataset['runtime_cat_141_170'] = df_training['runtime'].apply(lambda x: 1 if (x >140)&(x<=170) else 0)\n",
    "    training_dataset['runtime_cat_171_max'] = df_training['runtime'].apply(lambda x: 1 if (x >=170) else 0)\n",
    "    \n",
    "    validation_dataset['runtime_cat_min_60'] = df_validation['runtime'].apply(lambda x: 1 if (x <=60) else 0)\n",
    "    validation_dataset['runtime_cat_61_80'] = df_validation['runtime'].apply(lambda x: 1 if (x >60)&(x<=80) else 0)\n",
    "    validation_dataset['runtime_cat_81_100'] = df_validation['runtime'].apply(lambda x: 1 if (x >80)&(x<=100) else 0)\n",
    "    validation_dataset['runtime_cat_101_120'] = df_validation['runtime'].apply(lambda x: 1 if (x >100)&(x<=120) else 0)\n",
    "    validation_dataset['runtime_cat_121_140'] = df_validation['runtime'].apply(lambda x: 1 if (x >120)&(x<=140) else 0)\n",
    "    validation_dataset['runtime_cat_141_170'] = df_validation['runtime'].apply(lambda x: 1 if (x >140)&(x<=170) else 0)\n",
    "    validation_dataset['runtime_cat_171_max'] = df_validation['runtime'].apply(lambda x: 1 if (x >=170) else 0)\n",
    "    \n",
    "    # release date\n",
    "    training_dataset['release_date'] = pd.to_datetime(df_training['release_date'])\n",
    "    validation_dataset['release_date'] = pd.to_datetime(df_validation['release_date'])\n",
    "    \n",
    "    date_parts = [\"year\", \"weekday\", \"month\"]\n",
    "    for part in date_parts:\n",
    "        part_col = 'release_date' + \"_\" + part #add prefix as  \"release_date\" before the columne\n",
    "        training_dataset[part_col] = getattr(training_dataset['release_date'].dt, part).astype(int)\n",
    "        validation_dataset[part_col] = getattr(validation_dataset['release_date'].dt, part).astype(int)\n",
    "    training_dataset = training_dataset.drop(['release_date'], axis=1)\n",
    "    validation_dataset = validation_dataset.drop(['release_date'], axis=1)\n",
    "    \n",
    "    # keyword\n",
    "    # get 10 most appearing\n",
    "    temp = input_training[['movie_id','revenue','keywords']].copy()\n",
    "    temp['keywords'] = temp['keywords'].apply(lambda x: get_json_names(x))\n",
    "    keywords_list = list(temp['keywords'].apply(pd.Series).stack().value_counts().index)[:10]\n",
    "    \n",
    "    training_dataset['keywords'] = df_training['keywords'].apply(lambda x: get_json_names(x))\n",
    "    for keyword in keywords_list :\n",
    "        training_dataset['keyword_'+keyword]=training_dataset['keywords'].apply(lambda x: 1 if keyword in x else 0)\n",
    "    training_dataset = training_dataset.drop(['keywords'], axis=1)\n",
    "\n",
    "    validation_dataset['keywords'] = df_validation['keywords'].apply(lambda x: get_json_names(x))\n",
    "    for keyword in keywords_list :\n",
    "        validation_dataset['keyword_'+keyword]=validation_dataset['keywords'].apply(lambda x: 1 if keyword in x else 0)\n",
    "    validation_dataset = validation_dataset.drop(['keywords'], axis=1)\n",
    "    \n",
    "#     # production_countries\n",
    "#     # top 10 countries who produce most movies\n",
    "#     temp = input_training[['movie_id','revenue','production_countries']].copy()\n",
    "#     temp['production_countries'] = input_training['production_countries'].apply(lambda x: get_json_names(x))\n",
    "#     production_countries_list = list(temp['production_countries'].apply(pd.Series).stack().value_counts().index)[:10]\n",
    "    \n",
    "#     training_dataset['production_countries'] = df_training['production_countries'].apply(lambda x: get_json_names(x))\n",
    "#     for country in production_countries_list :\n",
    "#         training_dataset['country_'+country]=training_dataset['production_countries'].apply(lambda x: 1 if country in x else 0)\n",
    "#     training_dataset = training_dataset.drop(['production_countries'], axis=1)\n",
    "    \n",
    "#     validation_dataset['production_countries'] = df_validation['production_countries'].apply(lambda x: get_json_names(x))\n",
    "#     for country in production_countries_list :\n",
    "#         validation_dataset['country_'+country]=validation_dataset['production_countries'].apply(lambda x: 1 if country in x else 0)\n",
    "#     validation_dataset = validation_dataset.drop(['production_countries'], axis=1)\n",
    "    \n",
    "    \n",
    "#     # cast\n",
    "    temp = df_training[['movie_id','revenue','cast']].copy()\n",
    "    temp['cast'] = temp['cast'].apply(lambda x: get_json_names(x))\n",
    "    cast_list = list(temp['cast'].apply(pd.Series).stack().value_counts().index)[:10]\n",
    "    \n",
    "    training_dataset['cast'] = df_training['cast'].apply(lambda x: get_json_names(x))\n",
    "    for cast in cast_list :\n",
    "        training_dataset['cast_'+cast]=training_dataset['cast'].apply(lambda x: 1 if cast in x else 0)\n",
    "    training_dataset = training_dataset.drop(['cast'], axis=1)\n",
    "    \n",
    "    validation_dataset['cast'] = df_validation['cast'].apply(lambda x: get_json_names(x))\n",
    "    for cast in cast_list :\n",
    "        validation_dataset['cast_'+cast]=validation_dataset['cast'].apply(lambda x: 1 if cast in x else 0)\n",
    "    validation_dataset = validation_dataset.drop(['cast'], axis=1)\n",
    "    \n",
    "    \n",
    "# #     # spoken_languages\n",
    "#     temp = df_training[['movie_id','revenue','spoken_languages']].copy()\n",
    "#     temp['spoken_languages'] = temp['spoken_languages'].apply(lambda x: get_json_names(x))\n",
    "#     spoken_languages_list = list(temp['spoken_languages'].apply(pd.Series).stack().value_counts().index)[:10]\n",
    "    \n",
    "#     training_dataset['spoken_languages'] = df_training['spoken_languages'].apply(lambda x: get_json_names(x))\n",
    "#     for lang in spoken_languages_list :\n",
    "#         training_dataset['spoken_languages_'+lang]=training_dataset['spoken_languages'].apply(lambda x: 1 if lang in x else 0)\n",
    "#     training_dataset = training_dataset.drop(['spoken_languages'], axis=1)\n",
    "    \n",
    "#     validation_dataset['spoken_languages'] = df_validation['spoken_languages'].apply(lambda x: get_json_names(x))\n",
    "#     for lang in spoken_languages_list :\n",
    "#         validation_dataset['spoken_languages_'+lang]=validation_dataset['spoken_languages'].apply(lambda x: 1 if lang in x else 0)\n",
    "#     validation_dataset = validation_dataset.drop(['spoken_languages'], axis=1)\n",
    "    \n",
    "\n",
    "    return training_dataset, validation_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "biblical-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def model_training_and_prediction(training_dataset, validation_dataset):\n",
    "\n",
    "    \n",
    "    \n",
    "#     print(preds)\n",
    "    \n",
    "    return preds\n",
    "\n",
    "#     classifiers = [KNeighborsClassifier(),\n",
    "#                DecisionTreeClassifier(),\n",
    "#                LinearDiscriminantAnalysis(),\n",
    "#                LogisticRegression(),\n",
    "#                GaussianNB(),\n",
    "#                SVC()]\n",
    "\n",
    "#     classifier_accuracy_list = []\n",
    "#     for i, classifier in enumerate(classifiers):\n",
    "#         # split the dataset into 5 folds; then test the classifier against each fold one by one\n",
    "#         accuracies = cross_val_score(classifier, train_X, train_y, cv=5)\n",
    "#         classifier_accuracy_list.append((accuracies.mean(), type(classifier).__name__))\n",
    "\n",
    "#     # sort the classifiers\n",
    "#     classifier_accuracy_list = sorted(classifier_accuracy_list, reverse=True)\n",
    "#     for item in classifier_accuracy_list:\n",
    "#         print(item[1], ':', item[0])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "coordinated-police",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_training = pd.read_csv(\"training.csv\")\n",
    "input_validation = pd.read_csv(\"validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "cooked-functionality",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training, df_validation = format_training_data(input_training, input_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-chamber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "dental-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "train_X = df_training.drop(['revenue', 'movie_id', 'rating'], axis=1)\n",
    "test_X = df_validation.drop(['revenue', 'movie_id', 'rating'], axis=1)\n",
    "\n",
    "train_y = df_training['rating']\n",
    "test_y = df_validation['rating']\n",
    "\n",
    "\n",
    "# clf = LogisticRegression().fit(train_X, train_y)\n",
    "# clf = DecisionTreeClassifier().fit(train_X, train_y)\n",
    "clf = LinearDiscriminantAnalysis().fit(train_X, train_y)\n",
    "preds = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "protected-image",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2100 entries, 0 to 2099\n",
      "Data columns (total 85 columns):\n",
      " #   Column                                          Non-Null Count  Dtype\n",
      "---  ------                                          --------------  -----\n",
      " 0   budget_cat_min_30000000                         2100 non-null   int64\n",
      " 1   budget_cat_30000000_40000000                    2100 non-null   int64\n",
      " 2   budget_cat_40000000_50000000                    2100 non-null   int64\n",
      " 3   budget_cat_50000000_60000000                    2100 non-null   int64\n",
      " 4   budget_cat_60000000_70000000                    2100 non-null   int64\n",
      " 5   budget_cat_70000000_80000000                    2100 non-null   int64\n",
      " 6   budget_cat_80000000_90000000                    2100 non-null   int64\n",
      " 7   budget_cat_90000000_100000000                   2100 non-null   int64\n",
      " 8   budget_cat_100000000_130000000                  2100 non-null   int64\n",
      " 9   budget_cat_130000000_160000000                  2100 non-null   int64\n",
      " 10  budget_cat_160000000_190000000                  2100 non-null   int64\n",
      " 11  budget_cat_190000000_max                        2100 non-null   int64\n",
      " 12  has_homepage                                    2100 non-null   int64\n",
      " 13  en                                              2100 non-null   int64\n",
      " 14  ja                                              2100 non-null   int64\n",
      " 15  fr                                              2100 non-null   int64\n",
      " 16  zh                                              2100 non-null   int64\n",
      " 17  ko                                              2100 non-null   int64\n",
      " 18  te                                              2100 non-null   int64\n",
      " 19  ru                                              2100 non-null   int64\n",
      " 20  de                                              2100 non-null   int64\n",
      " 21  cn                                              2100 non-null   int64\n",
      " 22  es                                              2100 non-null   int64\n",
      " 23  it                                              2100 non-null   int64\n",
      " 24  nl                                              2100 non-null   int64\n",
      " 25  genre_Drama                                     2100 non-null   int64\n",
      " 26  genre_Action                                    2100 non-null   int64\n",
      " 27  genre_Comedy                                    2100 non-null   int64\n",
      " 28  genre_Thriller                                  2100 non-null   int64\n",
      " 29  genre_Adventure                                 2100 non-null   int64\n",
      " 30  genre_Crime                                     2100 non-null   int64\n",
      " 31  genre_Romance                                   2100 non-null   int64\n",
      " 32  genre_Science Fiction                           2100 non-null   int64\n",
      " 33  genre_Family                                    2100 non-null   int64\n",
      " 34  genre_Fantasy                                   2100 non-null   int64\n",
      " 35  company_Warner Bros.                            2100 non-null   int64\n",
      " 36  company_Universal Pictures                      2100 non-null   int64\n",
      " 37  company_Paramount Pictures                      2100 non-null   int64\n",
      " 38  company_Twentieth Century Fox Film Corporation  2100 non-null   int64\n",
      " 39  company_Columbia Pictures                       2100 non-null   int64\n",
      " 40  company_New Line Cinema                         2100 non-null   int64\n",
      " 41  company_Relativity Media                        2100 non-null   int64\n",
      " 42  company_Walt Disney Pictures                    2100 non-null   int64\n",
      " 43  company_Touchstone Pictures                     2100 non-null   int64\n",
      " 44  company_Village Roadshow Pictures               2100 non-null   int64\n",
      " 45  crew_Robert Rodriguez                           2100 non-null   int64\n",
      " 46  crew_Steven Spielberg                           2100 non-null   int64\n",
      " 47  crew_Hans Zimmer                                2100 non-null   int64\n",
      " 48  crew_Francine Maisler                           2100 non-null   int64\n",
      " 49  crew_James Newton Howard                        2100 non-null   int64\n",
      " 50  crew_Mary Vernieu                               2100 non-null   int64\n",
      " 51  crew_Deborah Aquila                             2100 non-null   int64\n",
      " 52  crew_Kevin Kaska                                2100 non-null   int64\n",
      " 53  crew_Dan O'Connell                              2100 non-null   int64\n",
      " 54  crew_Avy Kaufman                                2100 non-null   int64\n",
      " 55  runtime_cat_min_60                              2100 non-null   int64\n",
      " 56  runtime_cat_61_80                               2100 non-null   int64\n",
      " 57  runtime_cat_81_100                              2100 non-null   int64\n",
      " 58  runtime_cat_101_120                             2100 non-null   int64\n",
      " 59  runtime_cat_121_140                             2100 non-null   int64\n",
      " 60  runtime_cat_141_170                             2100 non-null   int64\n",
      " 61  runtime_cat_171_max                             2100 non-null   int64\n",
      " 62  release_date_year                               2100 non-null   int64\n",
      " 63  release_date_weekday                            2100 non-null   int64\n",
      " 64  release_date_month                              2100 non-null   int64\n",
      " 65  keyword_duringcreditsstinger                    2100 non-null   int64\n",
      " 66  keyword_based on novel                          2100 non-null   int64\n",
      " 67  keyword_aftercreditsstinger                     2100 non-null   int64\n",
      " 68  keyword_dystopia                                2100 non-null   int64\n",
      " 69  keyword_3d                                      2100 non-null   int64\n",
      " 70  keyword_violence                                2100 non-null   int64\n",
      " 71  keyword_woman director                          2100 non-null   int64\n",
      " 72  keyword_murder                                  2100 non-null   int64\n",
      " 73  keyword_sequel                                  2100 non-null   int64\n",
      " 74  keyword_revenge                                 2100 non-null   int64\n",
      " 75  cast_Samuel L. Jackson                          2100 non-null   int64\n",
      " 76  cast_Morgan Freeman                             2100 non-null   int64\n",
      " 77  cast_Bruce Willis                               2100 non-null   int64\n",
      " 78  cast_Robert De Niro                             2100 non-null   int64\n",
      " 79  cast_Matt Damon                                 2100 non-null   int64\n",
      " 80  cast_Owen Wilson                                2100 non-null   int64\n",
      " 81  cast_Johnny Depp                                2100 non-null   int64\n",
      " 82  cast_Liam Neeson                                2100 non-null   int64\n",
      " 83  cast_Brad Pitt                                  2100 non-null   int64\n",
      " 84  cast_Nicolas Cage                               2100 non-null   int64\n",
      "dtypes: int64(85)\n",
      "memory usage: 1.4 MB\n"
     ]
    }
   ],
   "source": [
    "train_X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "advanced-occasion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5333333333333333"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "baseline = [3]*400\n",
    "i = 0\n",
    "tp = 0\n",
    "\n",
    "fp = 0\n",
    "while (i < 400):\n",
    "    if preds[i] == df_validation.iloc[i]['rating']:\n",
    "        tp += 1\n",
    "\n",
    "    i+=1\n",
    "    \n",
    "tp/(tp+fp)\n",
    "    \n",
    "    \n",
    "# precision_recall_fscore_support(df_validation['rating'], preds, average='macro')\n",
    "# while (i < 400):\n",
    "#     if preds[i] == df_validation.iloc[i]['rating']:\n",
    "#         tp += 1\n",
    "#     i+=1\n",
    "    \n",
    "# print(tp/400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "frozen-market",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6531073446327684, 0.6391946229931613, 0.6442154336891179, None)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision_recall_fscore_support(df_validation['rating'], preds, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "following-hamilton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df_validation['rating'], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "durable-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pd.DataFrame(columns=['movie_id', 'predicted_rating'])\n",
    "output['movie_id'] = df_validation['movie_id']\n",
    "output['predicted_rating'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "prerequisite-allowance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19994</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25166</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12120</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9800</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1090</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>7942</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>73247</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>253331</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>9952</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>256924</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     movie_id  predicted_rating\n",
       "0       19994                 2\n",
       "1       25166                 2\n",
       "2       12120                 3\n",
       "3        9800                 3\n",
       "4        1090                 3\n",
       "..        ...               ...\n",
       "395      7942                 2\n",
       "396     73247                 3\n",
       "397    253331                 3\n",
       "398      9952                 3\n",
       "399    256924                 3\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-annual",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
